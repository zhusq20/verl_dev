class RayPPOTrainer:
    def _partial_rollout_add_from_dataloader_for_rollout(self, data_iter, replay_gen_batch):
        def get_batch_or_exhausted(data_iter, desired_size):
            batches = []
            accumulated_len = 0
            while accumulated_len < desired_size:
                try:
                    batch_dict = next(data_iter)
                    new_batch = DataProto.from_single_dict(batch_dict)
                    batches.append(new_batch)
                    accumulated_len += len(new_batch)
                except StopIteration:
                    break
            return DataProto.concat(batches) if len(batches) else None

        replay_partial_size = len(replay_gen_batch) if replay_gen_batch else 0
        n = self.config.actor_rollout_ref.rollout.n
        desired_num = self.config.data.train_batch_size - replay_partial_size // n
        new_batch = get_batch_or_exhausted(data_iter, desired_num)
        if new_batch is not None:
            # successfully added more data from dataloader
            # add metadata: is_partial and uid
            new_batch.batch["is_partial"] = torch.zeros((len(new_batch),), dtype=torch.bool)
            new_batch.non_tensor_batch["uid"] = np.array(
                [str(uuid.uuid4()) for _ in range(len(batch.batch))],
                dtype=object,
            )
            # merge with replay_gen_batch
            if batch is None:
                batch = new_batch
            else:
                # NOTE: here we put new_batch later, because if we clip the gen_batch
                # we deprioritize clipping partial rollout, so that its different tokens are
                # generated by the nearmost weights.
                batch = DataProto.concat([replay_gen_batch, new_batch])
        elif replay_partial_size > 0:
            # dataloader exhausted, use the remaining partial rollout for generation
            batch = replay_gen_batch
        else:
            # dataloader exhausted and no replay_gen_batch
            batch = None
        return batch, new_batch

    def _split_rollout_by_partial(self, batch: DataProto):
        tensor_batch = batch.batch
        partial_index = batch.batch["is_partial"].nonzero(as_tuple=True)[0]
        full_index = torch.logical_not(batch.batch["is_partial"]).nonzero(as_tuple=True)[0]
        partial_batch = self._data_proto_item_to_data_proto(batch[partial_index])
        full_batch = self._data_proto_item_to_data_proto(batch[full_index])
        return partial_batch, full_batch
    
    def _data_proto_item_to_data_proto(self, item):
        return DataProto(item.batch, item.non_tensor_batch, item.meta_info)

    def _post_process_partial_rollout(self, partial_rollouts: DataProto,
                                      replay_buffer_partial: DataProto):
        """post-process partial rollouts."""
        pad_token_id: int = self.tokenizer.pad_token_id
        # 1. gen_batch_output introduced new keys, we need to remove them
        prompts_and_response = partial_rollouts.pop(["prompts", "responses"])
        prompts = prompts_and_response.batch["prompts"]
        # 2. The value is current both left and right padded. We need to make it instead
        # only right padded. This applies for "input_ids", "attention_mask", "position_ids"
        for key in ["input_ids", "attention_mask", "position_ids"]:
            tensor = partial_rollouts.batch[key]
            # locate the pad token length at the end. Note that we don't count pad token at the beginning
            pad_length_full = (tensor == pad_token_id).sum(dim=-1, keepdim=True)
            # use the first-nonzero
            pad_length_left = (prompts == pad_token_id).sum(dim=-1, keepdim=True)
            pad_length_right = pad_length_full - pad_length_left
            # update the tensor by moving all padding to the left
            batch_size, seq_len = tensor.shape
            j = torch.arange(seq_len, device=tensor.device).unsqueeze(0).expand(batch_size, seq_len)
            indices = torch.where(
                j < pad_length_left,
                j,
                torch.where(
                    j < pad_length_left + pad_length_right,
                    j - pad_length_left + (seq_len - pad_length_right),
                    j - pad_length_right
                )
            )
            # Rearrange each row using the computed indices.
            tensor = torch.gather(tensor, 1, indices)
            partial_rollouts.batch[key] = tensor

        # 3. concatenate with existing replay buffer partial (Should not exist?)
        if replay_buffer_partial is None:
            replay_buffer_partial = partial_rollouts
        else:
            replay_buffer_partial = DataProto.concat([replay_buffer_partial, partial_rollouts])
        return replay_buffer_partial

    def fit(self):
        """
        The training loop of PPO.
        The driver process only need to call the compute functions of the worker group through RPC to construct the PPO dataflow.
        The light-weight advantage computation is done on the driver process.
        """
        from verl.utils.tracking import Tracking
        from omegaconf import OmegaConf

        logger = Tracking(
            project_name=self.config.trainer.project_name,
            experiment_name=self.config.trainer.experiment_name,
            default_backend=self.config.trainer.logger,
            config=OmegaConf.to_container(self.config, resolve=True),
        )

        self.global_steps = 0

        # load checkpoint before doing anything
        self._load_checkpoint()

        # perform validation before training
        # currently, we only support validation using the reward_function.
        if self.val_reward_fn is not None and self.config.trainer.get(
            "val_before_train", True
        ):
            val_metrics = self._validate()
            pprint(f"Initial validation metrics: {val_metrics}")
            logger.log(data=val_metrics, step=self.global_steps)
            if self.config.trainer.get("val_only", False):
                return

        # we start from step 1
        self.global_steps += 1

        for epoch in range(self.config.trainer.total_epochs):

            use_partial_rollout = getattr(self.config.rollout, "use_partial_rollout", False)
            if use_partial_rollout:
                assert (self.config.algorithm.adv_estimator
                        in ["gae", "reinforce_plus_plus"]), "partial rollout does not support adv_algo needs all samples"

                replay_buffer_partial: DataProto = None
                replay_buffer_full: DataProto = None
                data_iter = iter(self.train_dataloader)

                epoch_done: bool = False
                n = self.config.actor_rollout_ref.rollout.n
                while not epoch_done:
                    # start a new iteration.
                    replay_partial_size = len(replay_buffer_partial) if replay_buffer_partial else 0
                    replay_full_size = len(replay_buffer_full) if replay_buffer_full else 0

                    do_rollout_gen = False
                    new_batch = None
                    batch = None
                    # there is no enough data remain to be used for training, need to generate new rollouts
                    if replay_full_size < self.config.data.train_batch_size * n:
                        replay_gen_batch = replay_buffer_partial

                        # TODO: the train_batch_size here should be changed to another arg specialized for this.
                        if replay_partial_size < self.config.data.train_batch_size * n:
                            batch, new_batch = self._partial_rollout_add_from_dataloader_for_rollout(
                                data_iter, replay_gen_batch
                            )
                        else:
                            batch = replay_gen_batch

                        replay_buffer_partial = None    # replay buffer is consumed into the batch

                        if len(batch) > 0:
                            # handle batch not divisible by world_size
                            batch, gen_batch_pad_size = pad_dataproto_to_divisor(batch, self.actor_rollout_wg.world_size)
                            # Force it to only sample once to reduce the cost. TODO: maybe extend is_partial to int, and handle
                            # the special padding case so that it is not even computed?
                            batch.batch["is_partial"][-gen_batch_pad_size:] = torch.ones((gen_batch_pad_size,), dtype=torch.bool)

                            gen_batch = batch.pop(
                                batch_keys=["input_ids", "attention_mask", "position_ids", "is_partial"]
                            )
                        do_rollout_gen = len(batch) > 0

                    with _timer("step", timing_raw):
                        # generate a batch
                        with _timer("gen", timing_raw):
                            if do_rollout_gen:
                                gen_batch_output = self.actor_rollout_wg.generate_sequences(
                                    gen_batch
                                )
                        if self.config.algorithm.adv_estimator == "remax":
                            with _timer("gen_max", timing_raw):
                                if do_rollout_gen:
                                    gen_baseline_batch = deepcopy(gen_batch)
                                    gen_baseline_batch.meta_info["do_sample"] = False
                                    gen_baseline_output = (
                                        self.actor_rollout_wg.generate_sequences(
                                            gen_baseline_batch
                                        )
                                    )

                                    batch = batch.union(gen_baseline_output)
                                    reward_baseline_tensor = self.reward_fn(batch)
                                    reward_baseline_tensor = reward_baseline_tensor.sum(dim=-1)

                                    batch.pop(batch_keys=list(gen_baseline_output.batch.keys()))

                                    batch.batch["reward_baselines"] = reward_baseline_tensor

                                    del gen_baseline_batch, gen_baseline_output

                        # Now that all generation steps are done. We handle the gen_batch:
                        # If it is partial, we put it back to the replay_buffer_partial.
                        with _timer("gen_post_process", timing_raw):
                            if do_rollout_gen:
                                gen_batch_output = unpad_dataproto(gen_batch_output, pad_size=gen_batch_pad_size)

                                # handle new_batch with n > 1
                                if new_batch is not None:
                                    new_batch = new_batch.repeat(
                                        repeat_times=self.config.actor_rollout_ref.rollout.n,
                                        interleave=True,
                                    )
                                    if replay_gen_batch is not None:
                                        batch = DataProto.concat([replay_gen_batch, new_batch])
                                    else:
                                        batch = new_batch
                                else:
                                    batch = replay_gen_batch
                                batch = batch.union(gen_batch_output)
                        # apply replay_buffer_full
                        if replay_full_size > 0:
                            batch = DataProto.concat([replay_buffer_full, batch])
                        # split batch into partial(unfinished) and full rollout
                        partial_rollouts, full_rollouts = self._split_rollout_by_partial(batch)
                        replay_buffer_partial = self._post_process_partial_rollout(
                            partial_rollouts, replay_buffer_partial,
                        )
                        batch = full_rollouts
                        # if the batch is too large: put the rest into the replay_buffer_full
                        chunked_size = len(batch) - self.config.data.train_batch_size * n
                        if chunked_size > 0:
                            # NOTE: __getitem__ returns DataProtoItem. we need to convert it to DataProto
                            replay_buffer_full = self._data_proto_item_to_data_proto(batch[-chunked_size:])
                            batch = self._data_proto_item_to_data_proto(batch[:-chunked_size])
                        elif chunked_size < 0:
                            continue

                        # balance the number of valid tokens on each dp rank.
                        # Note that this breaks the order of data inside the batch.
                        # Please take care when you implement group based adv computation such as GRPO and rloo
                        self._balance_batch(batch, metrics=metrics)

                        # compute global_valid tokens
                        batch.meta_info["global_token_num"] = torch.sum(
                            batch.batch["attention_mask"], dim=-1
                        ).tolist()

                        # recompute old_log_probs
                        with _timer("old_log_prob", timing_raw):
                            old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)
                            batch = batch.union(old_log_prob)

                        if self.use_reference_policy:
                            # compute reference log_prob
                            with _timer("ref", timing_raw):
                                ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(
                                    batch
                                )
                                batch = batch.union(ref_log_prob)

                        # compute values
                        if self.use_critic:
                            with _timer("values", timing_raw):
                                values = self.critic_wg.compute_values(batch)
                                batch = batch.union(values)

                        with _timer("adv", timing_raw):
                            # compute scores. Support both model and function-based.
                            # We first compute the scores using reward model. Then, we call reward_fn to combine
                            # the results from reward model and rule-based results.
                            if self.use_rm:
                                # we first compute reward model score
                                reward_tensor = self.rm_wg.compute_rm_score(batch)
                                batch = batch.union(reward_tensor)

                            # we combine with rule-based rm
                            reward_tensor = self.reward_fn(batch)
                            batch.batch["token_level_scores"] = reward_tensor

                            # compute rewards. apply_kl_penalty if available
                            if not self.config.actor_rollout_ref.actor.get(
                                "use_kl_loss", False
                            ):
                                batch, kl_metrics = apply_kl_penalty(
                                    batch,
                                    kl_ctrl=self.kl_ctrl,
                                    kl_penalty=self.config.algorithm.kl_penalty,
                                )
                                metrics.update(kl_metrics)
                            else:
                                batch.batch["token_level_rewards"] = batch.batch[
                                    "token_level_scores"
                                ]

                            # compute advantages, executed on the driver process
                            batch = compute_advantage(
                                batch,
                                adv_estimator=self.config.algorithm.adv_estimator,
                                gamma=self.config.algorithm.gamma,
                                lam=self.config.algorithm.lam,
                                num_repeat=self.config.actor_rollout_ref.rollout.n,
                            )

                        # update critic
                        if self.use_critic:
                            with _timer("update_critic", timing_raw):
                                critic_output = self.critic_wg.update_critic(batch)
                            critic_output_metrics = reduce_metrics(
                                critic_output.meta_info["metrics"]
                            )
                            metrics.update(critic_output_metrics)

                        # implement critic warmup
                        if self.config.trainer.critic_warmup <= self.global_steps:
                            # update actor
                            with _timer("update_actor", timing_raw):
                                actor_output = self.actor_rollout_wg.update_actor(batch)
                            actor_output_metrics = reduce_metrics(
                                actor_output.meta_info["metrics"]
                            )
                            metrics.update(actor_output_metrics)

                        # validate
                        if (
                            self.val_reward_fn is not None
                            and self.config.trainer.test_freq > 0
                            and self.global_steps % self.config.trainer.test_freq == 0
                        ):
                            with _timer("testing", timing_raw):
                                val_metrics: dict = self._validate()
                            metrics.update(val_metrics)

                        if (
                            self.config.trainer.save_freq > 0
                            and self.global_steps % self.config.trainer.save_freq == 0
                        ):
                            with _timer("save_checkpoint", timing_raw):
                                self._save_checkpoint()

                    # collect metrics
                    metrics.update(
                        compute_data_metrics(batch=batch, use_critic=self.use_critic)
                    )
                    metrics.update(
                        compute_timing_metrics(batch=batch, timing_raw=timing_raw)
                    )

                    # TODO: make a canonical logger that supports various backend
                    logger.log(data=metrics, step=self.global_steps)

                    self.global_steps += 1

                    if self.global_steps >= self.total_training_steps:

                        # perform validation after training
                        if self.val_reward_fn is not None:
                            val_metrics = self._validate()
                            pprint(f"Final validation metrics: {val_metrics}")
                            logger.log(data=val_metrics, step=self.global_steps)
                        if (
                            self.config.trainer.save_freq > 0
                            and (self.global_steps - 1) % self.config.trainer.save_freq != 0
                        ):
                            with _timer("save_checkpoint", timing_raw):
                                self._save_checkpoint()
                        return

                continue
            # End of partial rollout